---
title: "Dead_Ends"
author: "Team5"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
 
library(ezids)

knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 

```

# ***Dead Ends and Data Trends: A Journey Through America's Recent Accident Stats***

An insightful journey into understanding the patterns behind road fatalities in the U.S. and creating actionable insights for a safer future.


# ***Project Overview***

Motor vehicle accidents are a leading cause of unintentional injury-related deaths in the U.S.  Using the 2022 FARS dataset, our analysis focuses on revealing trends and risk factors that contribute to fatal crashes.


Our first step is getting ready by loading the necessary packages and the data. 
```{r, include=T, results='asis',message=TRUE}
# Loading the necessary libraries
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(plotly)
library(car)
library(PMCMRplus)
library(here)
```


```{r, include=T, results='markup',message=TRUE}
# Loading the dataset

dataset <- read_csv("dataset.csv")

# Now viewing the structure of the dataset
str(dataset)
summary(dataset)
```

# 1. Data Preprocessing

## Next, we check for duplicate rows. Let's clear them out!
```{r, include=T, results='markup',message=TRUE}
dataset <- dataset %>% distinct()
```

## Moving on to handling missing values...
```{r, include=T, results='markup',message=TRUE}
missing_values <- colSums(is.na(dataset))
print(missing_values[missing_values > 0])
```

## Drop columns with missing values: x, y (duplicated), and tway_id2 (irrelevant)
```{r}
# Drop columns with missing values
dataset <- dataset %>% 
  select(
    -c("TWAY_ID2",
       "x",
       "y"))
```

*Dropping columns with duplicated meaning and irrelevant*
```{r, include=T, results='markup',message=TRUE}
dataset <- dataset %>% select(-STATE, -CITY, -COUNTY, -MONTH, -DAYNAME, -DAY_WEEK, -MINUTENAME, -ROUTE, -RUR_URB, -FUNC_SYS, -RD_OWNER, -NHS, -SP_JUR, -LATITUDENAME, -LONGITUDNAME, -MILEPT, -HARM_EV, -MAN_COLL, -MILEPTNAME, -RELJCT1, -RELJCT2, -TYP_INT, -REL_ROAD, -WRK_ZONE, -LGT_COND, -WEATHER, -SCH_BUS, -RAIL, -NOT_MIN, -ARR_MINNAME, -ARR_HOUR, -HOSP_MN, -SCH_BUSNAME, -RAILNAME, -PERNOTMVIT, -VE_FORMS, -PERSONS)

```

## Clean the column names
```{r}
# Lets rename columns for consistency
colnames(dataset)<- str_to_lower(colnames(dataset))
colnames(dataset) <- str_replace_all(colnames(dataset), " ", "_")
colnames(dataset)
```

## Convert variables into correct data types
```{r}

# Convert some variables into factor variables
dataset$rur_urbname <- factor(dataset$rur_urbname)
dataset$day_weekname <- factor(dataset$day_weekname, levels = c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
dataset$func_sysname <- factor(dataset$func_sysname)
dataset$lgt_condname <- factor(dataset$lgt_condname)
dataset$weathername <-factor(dataset$weathername)
dataset$statename <- factor(dataset$statename)

str(dataset)
```

## **Outlier detection and removal**
```{r, include=T, results='markup',message=TRUE}

columns <- c("ve_total", "peds", "permvit", "arr_min", "hosp_hr")

dataset_long <- dataset %>%
  select(all_of(columns)) %>%  # Select specified columns
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Plotting boxplots to visualize outliers
ggplot(dataset_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, outlier.size = 2) +
  facet_wrap(~ variable, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Boxplots of Selected Columns to Identify Outliers",
       x = "Variables",
       y = "Values")

remove_outliers <- function(df, columns) {
  df %>% mutate(across(all_of(columns), ~ ifelse(
    . < (quantile(., 0.25, na.rm = TRUE) - 1.5 * IQR(., na.rm = TRUE)) | 
    . > (quantile(., 0.75, na.rm = TRUE) + 1.5 * IQR(., na.rm = TRUE)), 
    NA, .))) %>%
    drop_na()
}
dataset <- remove_outliers(dataset, columns)

# Reshape data to long format for boxplot visualization after removing outliers
dataset_long <- dataset %>%
  select(all_of(columns)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Plotting boxplots to visualize the cleaned data without outliers
ggplot(dataset_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, outlier.size = 2) +
  facet_wrap(~ variable, scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Boxplots of Selected Columns After Outlier Removal",
       x = "Variables",
       y = "Values")

```



#```{r, include=T, results='markup',message=TRUE}
#write.csv(dataset, "cleaned_dataset.csv", row.names = FALSE)
#```

```{r, include=T, results='markup',message=TRUE}
road_data <- dataset %>% 
  select(rur_urbname) %>%
  drop_na() %>%
  filter(!rur_urbname %in% c("Unknown", "Not Reported", "Trafficway Not in State Inventory")) %>%
  mutate(
    rur_urbname = factor(rur_urbname, labels = c("Rural", "Urban"))
  )
```

## Overview of Analysis
- Time Analysis (month, day of week, hour of day)
- Environmental Analysis(Weather/Lighting)
- Infrastructure Analysis

# 2. Temporal Analysis (Hour, Day of Week)

**Research Question 1: How do _temporal factors_ affect fatal motor vehicle accidents?**

- SMART question 1: How do **day of week** and **hour of day** affect the occurrence and severity of fatal accidents?

- SMART question 2: Does **day of week** affect the day to day variation on total fatalities per day?


## Data Subsetting and Preparation
Subset for temporal question
```{r Q1:Create Temporal Subset}
# Select columns consists of temporal factors (month, day, day_week, hour) and no of fatalities (fatals)
df_temporal <- dataset %>% 
  select("monthname",
         "day",
         "day_weekname",
         "hour",
         "fatals")

# Create a new column to stored the date
df_temporal <- df_temporal %>%
  mutate(
    date = as.Date(paste("2022", monthname, day, sep = "-"), format = "%Y-%b-%d"),
    day_type = ifelse(day_weekname %in% c("Saturday", "Sunday"), "Weekend", "Weekday"),
   fatal_category = ifelse(fatals > 1, "multiple", "one"),
    time_of_day = case_when(
    hour < 6 ~ "Early Morning",
    hour >= 6 & hour <12 ~ "Morning",
    hour >= 12 & hour < 18 ~ "Afternoon",
    hour >= 18 ~ "Evening")) 

# convert into factor variable and set up level
df_temporal$day_weekname <- factor(df_temporal$day_weekname, levels = c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
df_temporal$time_of_day <- factor(df_temporal$time_of_day, levels = c("Early Morning", "Morning", "Afternoon", "Evening"))

```

Check if every date in 2022 is included in the data frame, all dates in 2022 are included.
```{r eval=FALSE, include=FALSE}
# Generate all dates for the year 2022
all_dates_2022 <- seq.Date(from = as.Date("2022-01-01"), to = as.Date("2022-12-31"), by = "day")

# Extract unique dates from the accident table
accident_dates <- unique(df_temporal$date)

# Find dates in 2022 that are missing from the accident table
missing_dates <- setdiff(all_dates_2022, accident_dates)
missing_dates # none: all included
```

Frequency Analysis: Day of Week and Weekday vs Weekend
This section calculates the frequency of occurrences for each day of the week and categorizes the results into weekdays and weekends. 

**Day of Week Analysis**: Sunday to Friday each has frequency of 52 occurrences, while Saturday has 53. The distribution across the days is balanced.

**Weekday vs Weekend Analysis**: Weekdays have a total of 260 counts, and weekends have 105 counts.

```{r include=FALSE}
# Calculate for the frequency of day of week in the data frame
unique_dates <- df_temporal %>%
  select(date, day_weekname) %>%
  distinct()
unique_dates[order(unique_dates$date), ]

# Count the frequency of day of week
day_of_week_counts <- unique_dates %>%
  group_by(day_weekname) %>%
  summarise(count = n())
day_of_week_counts

# Count the frequency of Weekday and weekend
weekday_weekend_counts <- day_of_week_counts %>%
  mutate(category = ifelse(day_weekname %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))

# Summarize the total counts for Weekdays and Weekends
total_weekday_weekend_counts <- weekday_weekend_counts %>%
  group_by(category) %>%
  summarise(total_count = sum(count))
total_weekday_weekend_counts

```


## 2.1 Accident Occurence Analysis
### 2.1.1 Summary Statistics
#### Hour of Day
```{r}
# Filter out unknown hour (hour==99)
df_hour <- df_temporal %>% 
  filter(hour != 99)

df_hour_accidents <- df_hour %>% 
  group_by(hour) %>% 
  summarise(total_accidents = n())%>% 
  ungroup()
df_hour_accidents

# Top 5 hours with the highest number of fatal accidents
#head(df_hour_accidents[order(df_hour_accidents$total_accidents, decreasing = TRUE), ], 5)
# Top 5 hours with the lowest number of fatal accidents
#head(df_hour_accidents[order(df_hour_accidents$total_accidents), ], 5)
```

The peak hours for the highest number of accidents occur between 6 PM and 10 PM
#### Day of Week
```{r}
df_wday_accidents <- df_temporal %>% 
  group_by(day_weekname) %>% 
  summarise(total_accidents = n())%>% 
  ungroup()
df_wday_accidents
```

Saturday and Sunday appear to have higher count of fatal accidents than rest of days of week.
 
### 2.1.2 Data Visualization
#### Hour of Day
```{r}
# Count of Fatal Accidents
df_hour %>% ggplot(aes(x= hour, fill = time_of_day)) +
  geom_bar()+
  labs(x = "hour", y = "Number of Fatal Accidents", 
       title = "Count of Fatal Accidents by Hour",
       fill = "time of day")+
  scale_fill_manual(values = c("#999999","#E69F00", "#D55E00", "#0072B2"))+
  theme_minimal()


```

Given the patterns in hour of day, we will categorize day into two time group: Early morning, morning, afternoon, and evening.

#### Day of Week
```{r}

df_wday_accidents <- df_wday_accidents %>%
  mutate(day_type = ifelse(day_weekname %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))


# Data Visualization on Occurrence of fatal accidents per day of week
df_temporal %>% ggplot(aes(x= day_weekname, fill = day_type)) +
  geom_bar()+
  labs(x = "Day of the Week", y = "Number of Fatal Accidents", 
       title = "Frequency of Fatal Accidents by Day of the Week",
       fill = "Day of Week") +  
  scale_fill_manual(values = c("#bfbfbf", "#0072B2"), 
                    labels = c("weekend", "weekday")) +
  theme_minimal()

```

### 2.1.3 Statistical Testing: 
We use Chi-squared test to determine whether accidents are uniformly distributed across time of the day/days of week. 
#### Hour of Day
Observed from the patterns of occurrence of accidents across hours of day and the usual practice, we will divide the hour of day into four groups.

```{r Hour of Day}
# chi-squared test on the occurrence of accident
table(df_hour$time_of_day)
chisq_result <- chisq.test(table(df_hour$time_of_day))
chisq_result

# Get observed and expected counts
observed <- chisq_result$observed
expected <- chisq_result$expected

# Calculate standardized residuals
standardized_residuals <- (observed - expected) / sqrt(expected)
print(standardized_residuals)
```

#### Day of Week

In the data set, six days have 52 occurrences and one has 53. Even though the days are not perfectly equal but the impact will likely be minimal given the small difference between 52 and 53. For simplicity and common assumption, I will be using equal distribution across all seven days in the test. 

```{r : Statistical Testing}
# chi-squared test on the occurrence of accident
table(df_temporal$day_weekname)

chi_sq_test <- chisq.test(table(df_temporal$day_weekname))
print(chi_sq_test)

# Get observed and expected counts
observed <- chi_sq_test$observed
expected <- chi_sq_test$expected

# Calculate standardized residuals
standardized_residuals <- (observed - expected) / sqrt(expected)
print(standardized_residuals)
```

P-value for both test are less than 0.05, indicating that there is a statistically significant deviation from a uniform distribution, suggesting that certain hours and days have higher frequencies of accidents compared to others.

## 2.2 Fatality Analysis
Since the majority of accident has one fatality in an accident and limited occurrence on accidents with fatalities more than four, we will group the number of fatalities into *"one"* and *"multiple"* fatalities.

### 2.2.1 Summary Statistics
#### Hour of Day
```{r}
hr_contingency <- table(df_hour$time_of_day, df_hour$fatal_category)
hr_contingency
```
#### Day of Week
```{r}
# Severity: categorize accidents into accident with one fatality and multiple
fatality_counts <- df_temporal %>%
  group_by(day_weekname, fatal_category) %>%
  summarize(count = n())
df_temporal

wday_contingency <- table(df_temporal$day_weekname, df_temporal$fatal_category)
wday_contingency
```
### 2.2.2 Data Visualization
#### Hour of Day
```{r fig.width=12, fig.height=6}
# Convert contingency table into data frame and rename column names
df_hr_contingency <- data.frame(hr_contingency)
colnames(df_hr_contingency)<- c("time_of_day", "fatal_category", "Freq")

df_hr_contingency

#data viz
grouped_hr_contingency <- df_hr_contingency %>%
  group_by(time_of_day) %>%
  mutate(proportion = Freq / sum(Freq))
grouped_hr_contingency

grouped_hr_contingency %>% 
  ggplot(aes(x= time_of_day, y=proportion, fill = fatal_category))+
  geom_bar(stat="identity", alpha = 0.75) +
  facet_wrap(~ fatal_category,scales = "free_y")
```

#### Day of Week
```{r}
# Convert contingency table into data frame and rename column names
df_wday_contingency <- data.frame(wday_contingency)
colnames(df_wday_contingency)<- c("day_weekname", "fatal_category", "Freq")

# data visualization on Severity by day of week
# Frequency Counts
#df_wday_contingency %>% ggplot(aes(x= day_weekname, y = Freq, fill = fatal_category))+
#  geom_bar(stat="identity", alpha = 0.75) +
#  facet_wrap(~ fatal_category)+
#  labs(x = "Day of the Week", y = "Frequency") +
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Proportion 
grouped_wday_contingency <- df_wday_contingency %>%
  group_by(day_weekname) %>%
  mutate(proportion = Freq / sum(Freq))
grouped_wday_contingency

grouped_wday_contingency %>% 
  ggplot(aes(x= day_weekname, y=proportion, fill = fatal_category))+
  geom_bar(stat="identity", alpha = 0.75) +
  facet_wrap(~ fatal_category,  scales = "free_y") +
    labs(x = "Day of the Week", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### 2.2.3 Statistical Testing
#### Hour of Day
```{r}
# Chi-squared test: hour of day
chi_sq_result <- chisq.test(hr_contingency)
print(chi_sq_result)

# standardized residuals table
# get the expected and observed values
observed <- chi_sq_result$observed
expected <- chi_sq_result$expected
# calculate the standardized residuals
standardized_residuals <- (observed - expected) / sqrt(expected)
print(standardized_residuals)

```
P-value less than 0.05, we can reject the null hypothesis at 5% level of significance. This indicates a significant relationship between the number of fatalities per accident ('one' vs. 'multiple') and the time of the day. 

#### Day of Week
```{r}
chi_sq_test<- chisq.test(wday_contingency)
print(chi_sq_test)

# Get observed and expected counts
observed <- chi_sq_test$observed
expected <- chi_sq_test$expected

# Calculate standardized residuals
standardized_residuals <- (observed - expected) / sqrt(expected)
print(standardized_residuals)
```

The result reveals a clear pattern in the data, highlighting that weekends are associated with more severe accidents involving multiple fatalities. 


## 2.3 Daily Fatalities

Look further into total fatalities per day to provide a more nuanced undertanding on how does temporal factors affect fatal accidents.
```{r}
# Data frame for daily fatalities
df_daily <- df_temporal %>%
  group_by(date, day_weekname) %>%
  summarise(
    total_accidents = n(),
    average_fatals = mean(fatals),
    total_fatals = sum(fatals)
  ) %>% 
  ungroup()


df_daily <- df_temporal %>%
  group_by(date, day_weekname) %>%
  summarise(
    total_accidents = n(),
    average_fatals = mean(fatals),
    total_fatals = sum(fatals)
  ) %>% 
  ungroup()

# convert to the right data type
df_daily$day_weekname <- factor(df_daily$day_weekname, levels =  c("Sunday","Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

```

### Top 5 Dates with Highest and Lowest Fatalities
```{r}
# Top 5 Highest Daily Fatalities
head(df_daily[order(df_daily$total_fatals, decreasing = TRUE), ], 5)
# Top 5 Lowest Daily Fatalities
tail(df_daily[order(df_daily$total_fatals, decreasing = TRUE), ], 5)
```

### 2.3.1 Data Visualization:Daily fatalities by day of week
```{r Day of Week: data viz}
# names of days of week
df_daily


df_daily <- df_daily %>%
  mutate(day_type = ifelse(day_weekname %in% c("Saturday", "Sunday"), "Weekend", "Weekday"))


# Boxplot of daily fatlities by day of week
df_daily %>% ggplot(aes(x = day_weekname, y = total_fatals, fill = day_type))+
  geom_boxplot() +
  labs(x = "Day of the Week", 
       y = "Daily Fatalities", 
       title = "Daily Fatalities by Day of the Week")+
  scale_fill_manual(values = c("#bfbfbf","#0072B2"))


```

The boxplot shows that daily fatalities are generally higher on weekends. This suggests that weekends tend to have a higher count of fatalities, potentially due to increased travel.

## 2.3.2 Summary Statistics of Day of Week
```{r avg daily fatalities by day of week}
 df_summary_day_week <- df_daily %>% 
  group_by(day_weekname) %>% 
  summarise(
    sample_size = n(),
    avg_fatalities = mean(total_fatals),
    std_fatalities = sd(total_fatals)) %>% 
  ungroup()

df_summary_day_week
```

The average and standard deviation of daily fatalities are notably higher on weekends, indicating both a higher mean and greater variability in fatalities compared to weekdays. The sample size is balanced across groups.

### 2.3.3 Check for Normality and Homogeneity of Variance
```{r}
# QQ plot
for(day in unique(df_daily$day_weekname)) {
  qqnorm(df_daily$total_fatals[df_daily$day_weekname == day], 
         main = paste("Q-Q Plot for ", day))
  qqline(df_daily$total_fatals[df_daily$day_weekname == day], col = "blue")
}


# Shapiro: Normality test
df_daily %>%
  group_by(day_weekname) %>%
  summarise(normality_p_value = shapiro.test(total_fatals)$p.value)
# Levene test: Homogeneiety of Variance
leveneTest(total_fatals ~ day_weekname, data = df_daily)
```

Since the number of fatalities per day is relatively large and spans a wide range of values, the daily fatality counts approximate a continuous variable. Combined with sufficiently large sample sizes for each day of the week, the Central Limit Theorem applies, making the sampling distribution of the mean fatalities per day approximately normal. Additionally, normality tests on daily fatalities grouped by day of the week do not reject the assumption of normality. Therefore, we can treat fatalities per day as approximately continuous and appropriately use ANOVA to test for differences across the days of the week

After doing the Leven's test for homogeneity, we can conclude that the variances across the groups are significantly different at 5% level of significance. This indicates that the homogeneity assumption of ANOVA test is violated. So we will be using Welch's ANOVA test in the following section

### 2.3.4 Statistical Testing
```{r Day of Week: ANOVA}
# Welch's ANOVA
Wanova_result <- oneway.test(total_fatals ~ day_weekname, data = df_daily, var.equal = FALSE)
Wanova_result
# post hoc test-Games Howell

posthoc_result <- gamesHowellTest(total_fatals ~ day_weekname, data = df_daily)
print(posthoc_result)
```

p-values <0.05, there is a statistically significant difference (p-value < 0.05) in the number of daily fatalities across different days of the week. This indicates that day of week can affect daily fatality counts.


# 3. Environmental Analysis (Lighting & Weather)

**Let's summarize the data to calculate the total fatalities by state.**

```{r, include=T, results='markup',message=TRUE}

state_county_summary <- dataset %>%
  group_by(statename, countyname) %>%
  summarise(Total_Fatalities = sum(fatals)) %>%
  ungroup()

state_summary <- state_county_summary %>%
  group_by(statename) %>%
  summarise(Total_Fatalities = sum(Total_Fatalities)) %>%
  arrange(desc(Total_Fatalities))

# 1. State-Level Analysis: Plot top 10 states by total fatalities
top_state_summary <- state_summary %>%
  slice_head(n = 10)  # Select top 10 states by total fatalities

state_bar_plot <- ggplot(top_state_summary, aes(x = reorder(statename, Total_Fatalities), y = Total_Fatalities, fill = Total_Fatalities)) +
  geom_bar(stat = "identity", color = "black", width = 0.7) +
  geom_text(aes(label = Total_Fatalities), hjust = -0.2, color = "black", size = 4) +
  labs(title = "Top 10 States by Total Fatalities",
       x = "State",
       y = "Total Number of Fatalities") +
  theme_minimal() +
  scale_fill_gradient(low = "lightblue", high = "darkred") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "none"
  ) +
  coord_flip()  # Horizontal bar plot


state_plotly <- ggplotly(state_bar_plot, tooltip = c("x", "y"))
state_plotly

# 2. County-Level Analysis: Plot top 10 counties by fatalities within a selected state
selected_state <- "Texas"  

county_data <- state_county_summary %>%
  filter(statename == selected_state) %>%
  arrange(desc(Total_Fatalities)) %>%
  slice_head(n = 10)

county_bar_plot <- ggplot(county_data, aes(x = reorder(countyname, Total_Fatalities), y = Total_Fatalities, fill = Total_Fatalities)) +
  geom_bar(stat = "identity", color = "black", width = 0.7) +
  geom_text(aes(label = Total_Fatalities), hjust = -0.2, color = "black", size = 4) +
  labs(title = paste("Top 10 Counties by Total Fatalities in", selected_state),
       x = "County",
       y = "Total Number of Fatalities") +
  theme_minimal() +
  scale_fill_gradient(low = "lightyellow", high = "darkorange") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "none"
  ) +
  coord_flip()  

county_plotly <- ggplotly(county_bar_plot, tooltip = c("x", "y"))
county_plotly

```



**Research Question 2: What is the relationship between weather and lighting conditions on the occurrence of fatal accidents?**

## 3.1 **Bivariate Analysis**

```{r, include=T, results='markup',message=TRUE}
library(dplyr)
library(ggplot2)
library(plotly)
library(scales)

# Filtering out unknown or not reported values in the dataset
dataset <- dataset %>%
  filter(!(weathername %in% c("Not Reported", "Reported as Unknown"))) %>%
  filter(!(lgt_condname %in% c("Not Reported", "Reported as Unknown")))

# Aggregate data for weather conditions
weather_data <- dataset %>%
  group_by(weathername) %>%
  summarise(Total_Fatalities = sum(fatals, na.rm = TRUE)) %>%
  arrange(desc(Total_Fatalities))

lighting_data <- dataset %>%
  group_by(lgt_condname) %>%
  summarise(Total_Fatalities = sum(fatals, na.rm = TRUE)) %>%
  arrange(desc(Total_Fatalities))

# 1. Plot for Weather Conditions with Log Scale and Original Value Labels
weather_plot <- ggplot(weather_data, aes(x = reorder(weathername, Total_Fatalities), y = Total_Fatalities)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  scale_y_log10() + 
  labs(title = "Fatalities by Weather Conditions", x = "Weather Condition", y = "Total Fatalities (Log Scale)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_text(aes(label = comma(Total_Fatalities)), 
            hjust = -0.2, size = 3, color = "black")  


weather_plotly <- ggplotly(weather_plot, tooltip = c("x", "y"))
weather_plotly

# 2. Plot for Lighting Conditions with Log Scale and Original Value Labels
lighting_plot <- ggplot(lighting_data, aes(x = reorder(lgt_condname, Total_Fatalities), y = Total_Fatalities)) +
  geom_bar(stat = "identity", fill = "orange") +
  coord_flip() +
  scale_y_log10() + 
  labs(title = "Fatalities by Lighting Conditions", x = "Lighting Condition", y = "Total Fatalities (Log Scale)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_text(aes(label = comma(Total_Fatalities)),  
            hjust = -0.2, size = 3, color = "black") 
lighting_plotly <- ggplotly(lighting_plot, tooltip = c("x", "y"))

lighting_plotly

```

#### Clear weather has the highest number of fatalities, likely due to high exposure as most driving happens in clear conditions.

#### Cloudy and Rain also pose significant risks, while severe conditions like Snow, Fog, and Crosswinds still contribute to fatalities, albeit at lower rates.

#### This insight helps emphasize that while severe weather conditions contribute to road fatalities, most incidents occur during clear weather, likely due to the high volume of travel under these conditions.

## 3.2 **Multivariate analysis** : Let's analyze fatality counts by both weather and lighting conditions using a heatmap

```{r, include=T, results='markup',message=TRUE}
# Aggregating fatal counts by weather and lighting
weather_light <- dataset %>%
  group_by(weathername, lgt_condname) %>%
  summarize(total_fatals = sum(fatals, na.rm = TRUE))

# Interactive Heatmap
heatmap_plot <- plot_ly(weather_light, x = ~weathername, y = ~lgt_condname, z = ~total_fatals, type = "heatmap") %>%
  layout(title = "Fatality Count by Weather and Lighting Condition",
         xaxis = list(title = "Weather Condition"),
         yaxis = list(title = "Lighting Condition"))

heatmap_plot
```

#### **Clear and Daylight Conditions**: These conditions have the highest fatality counts, likely due to the higher volume of traffic under these conditions.

#### **Clear and Dark - Not Lighted**: The relatively high fatality count here suggests the need for improved street lighting on roads frequently used at night.

#### **Weather Impact**: Rain and Cloudy conditions contribute to fatalities across various lighting conditions, pointing to the increased risk associated with low-visibility and wet surfaces.


### ***This heatmap can help guide targeted safety measures:***

#### **Enhanced Lighting**: Roads that are dark and not lighted could benefit from additional lighting to reduce nighttime crashes, especially under clear conditions.

#### **Weather-Specific Warnings**: Increased signage or public warnings during rainy or cloudy weather may help drivers exercise caution, particularly in low-light conditions.


## **3.3 Location-Based Analysis**

### **Top 10 States by Fatal Accidents in Adverse Weather or Poor Lighting**
```{r, include=T, results='markup',message=TRUE}
state_analysis <- dataset %>%
  filter(weathername %in% c("Rain", "Sleet or Hail", "Blowing Snow","Blowing Sand,Soil Dirt","Severe Crosswinds","Fog,Smog,Smoke","Freezing Rain or Drizzle","Snow"), lgt_condname %in% c("Dark-Not Lighted", "Dark - Lighted","Dark- Unknown Lighting")) %>%
  group_by(statename) %>%
  summarise(Total_Fatalities = sum(fatals, na.rm = TRUE)) %>%
  arrange(desc(Total_Fatalities))


# Select the top 10 states with the highest fatalities
top_adverse_states <- head(state_analysis, 10)  

ggplot(top_adverse_states, aes(x = reorder(statename, Total_Fatalities), y = Total_Fatalities, fill = Total_Fatalities)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Top 10 States by Fatal Accidents in Adverse Weather or Poor Lighting",
       x = "State", y = "Total Fatalities") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10),  
        legend.position = "none")  
```

#### This chart shows the top 10 U.S. states with the highest number of fatal accidents occurring under adverse weather or poor lighting conditions. The states are ordered by the total number of fatalities, with Florida and Texas experiencing the highest counts. 

#### **Future Implications:** This analysis helps identify locations that could benefit from targeted safety measures in adverse weather or poor lighting, such as improved lighting infrastructure, better signage, or increased public awareness campaigns during severe weather events.

### **Top 10 States by Fatal Accidents in Clear Weather and Good lighting**

```{r, include=T, results='markup',message=TRUE}
state_analysis <- dataset %>%
  filter(weathername %in% c("Clear","Cloudy"), lgt_condname %in% c("Daylight", "Dawn","Dusk")) %>%
  group_by(statename) %>%
  summarise(Total_Fatalities = sum(fatals, na.rm = TRUE)) %>%
  arrange(desc(Total_Fatalities))


# Select the top 10 states with the highest fatalities
top_states <- head(state_analysis, 10)  

ggplot(top_states, aes(x = reorder(statename, Total_Fatalities), y = Total_Fatalities, fill = Total_Fatalities)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "States with highest Fatal Accidents in clear weather and good lighting",
       x = "State", y = "Total Fatalities") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10), 
        legend.position = "none")  
```

#### Despite favorable weather and lighting conditions, fatalities remain high in clear weather in states like California, Texas, and Florida. This suggests that driver behavior, traffic congestion, and infrastructure quality could be key factors influencing accident rates in these states.

#### However, Focused interventions, such as better road lighting, clear weather warnings, and infrastructure improvements to handle severe weather, could benefit states like Florida, Texas, and New York.


## **3.4 Statistical Tests**
#### Let us support the above results by performing statistical tests:

#### **ANOVA (Analysis of Variance) test** followed by a Tukey's HSD  post hoc test to analyze the impact of lighting conditions (lgt_condname) on the fatal accident counts (fatals) 

```{r, include=T, results='markup',message=TRUE}

anova_lighting <- aov(fatals ~ lgt_condname, data = dataset)

summary(anova_lighting)
```

#### Let us define our null and alternate hypothesis

#### **Null Hypothesis (H0)**: There is no difference in the mean number of fatalities across different lighting conditions (i.e., lighting conditions do not affect fatal accident counts).

#### **Alternative Hypothesis (H1)**: There is a difference in the mean number of fatalities across at least one pair of lighting conditions.


#### ***p-value:***

#### The p-value for lgt_condname is 0.0054, which is below the common significance level of 0.05.

#### This indicates that we reject the null hypothesis and conclude that there is a statistically significant difference in the mean fatal accident counts across different lighting conditions.

#### ***F-Statistic:***

#### The F-value of 2.72 suggests that the variance between the groups (lighting conditions) is more than what would be expected by chance, reinforcing that lighting conditions do affect fatal accident counts.

#### ***Sum of Squares:***

#### The sum of squares (SS) values indicate the variance within the groups (Residuals) and the variance attributed to the lighting conditions (lgt_condname).

#### The ANOVA results indicate that lighting conditions significantly impact the number of fatal accidents, meaning that certain lighting conditions are associated with different fatality rates.

#### Next Step with **Tukey's HSD Test**: Since the ANOVA indicates a significant effect, the Tukey's HSD test can be used to pinpoint which specific lighting conditions differ from each other in terms of fatal accident counts. 

```{r, include=T, results='markup',message=TRUE}
tukey_results <- TukeyHSD(anova_lighting, "lgt_condname")
print(tukey_results)

```

### ***Significant Comparisons:***

#### Only the comparison "Dark - Not Lighted" vs. "Dark - Lighted" has a p-value of 0.001, which is below 0.05. This suggests a statistically significant difference in mean fatalities between "Dark - Not Lighted" and "Dark - Lighted" conditions.

#### The positive diff value (0.02067) indicates that "Dark - Not Lighted" has a higher mean fatality count than "Dark - Lighted".


### ***Practical Implications :***

#### "Dark - Not Lighted" vs. "Dark - Lighted": Since there is a statistically significant difference between these two conditions, with "Dark - Not Lighted" associated with higher fatality counts, it may suggest that insufficient lighting in dark conditions contributes to a higher risk of fatal accidents.

#### **Recommendation**: Improving lighting in areas that are "Dark - Not Lighted" may help reduce fatal accidents, as lighting seems to play a crucial role in accident prevention under dark conditions.

#### For other lighting conditions, the lack of significant differences suggests that fatalities do not vary considerably across these conditions. This could imply that lighting in conditions like Dawn, Daylight, and Dusk might be sufficient, and improvements in these areas may not have as strong an impact on reducing fatal accidents.


#### **Independent t-test** (assuming the fatality rates in adverse and clear weather conditions are from two separate groups).

```{r, include=T, results='markup',message=TRUE}

# Filter data for adverse and clear weather conditions
adverse_conditions <- c("Rain", "Sleet or Hail", "Blowing Snow","Blowing Sand,Soil Dirt","Severe Crosswinds","Fog,Smog,Smoke","Freezing Rain or Drizzle","Snow")
fair_conditions <- c("Clear","Cloudy")

# Summarize fatalities by state for adverse weather conditions
adverse_weather <- dataset %>%
  filter(weathername %in% adverse_conditions) %>%
  group_by(statename) %>%
  summarise(Adverse_Fatalities = sum(fatals, na.rm = TRUE))

# Summarize fatalities by state for fair weather condition
fair_weather <- dataset %>%
  filter(weathername == fair_conditions) %>%
  group_by(statename) %>%
  summarise(Clear_Fatalities = sum(fatals, na.rm = TRUE))

# Merge the data for adverse and clear weather fatalities by state
weather_comparison <- merge(adverse_weather, fair_weather, by = "statename", all = TRUE)

# Replace NA values with 0 (in case a state has no fatalities in one of the conditions)
weather_comparison[is.na(weather_comparison)] <- 0

# Perform the independent t-test
t_test_result <- t.test(weather_comparison$Adverse_Fatalities, weather_comparison$Clear_Fatalities, 
                        alternative = "two.sided", var.equal = FALSE)

print(t_test_result)
```

#### **p-value**:

#### The p-value is 1e-05, which is extremely low (below the standard significance level of 0.05).
#### This indicates that we can reject the null hypothesis and conclude that there is a statistically significant difference in fatality rates between adverse weather and clear weather conditions.

#### **t-Statistic**:

#### The t-value of -5 suggests a substantial difference between the two groups. The negative value indicates that the mean fatality rate in adverse weather is lower than that in clear weather.

#### The results hence indicate a significant difference in fatality rates between adverse and clear weather conditions, with higher fatalities observed in clear weather.

#### This outcome could suggest that during clear weather, there may be higher traffic volumes, faster speeds, or other risk factors that increase the likelihood of fatalities, whereas adverse weather might lead drivers to exercise more caution, resulting in fewer fatalities.

#### Hence it is important to underastand that, in addition to weather and lighting conditions, there are several other critical factors like road infrastructure, juction types and driver behaviour which can equally influence the likelihood and severity of accidents. Addressing these factors can provide a more holistic approach to improving road safety and reducing fatalities.


# 4. Infrastructure Analysis

**Research Question 3: How do road conditions (urban/rural, road type, and traffic controls) impact the severity of crashes?** 

### Before analyzing this, let's prep the data

```{r, include=T, results='markup',message=TRUE}

road_fatalities <- dataset %>%
  filter(func_sysname != "Unknown" & func_sysname != "Not Reported" & func_sysname != "Trafficway Not in State Inventory") %>%
  select(func_sysname, fatals) %>%
  drop_na() %>%
  mutate(func_sysname = as.factor(func_sysname))

fatality_summary <- road_fatalities %>%
  group_by(func_sysname) %>%
  summarise(
    Total_Fatalities = sum(fatals),
    SD_Fatalities = sd(fatals),  # Standard deviation for reference
    Count = n()
  )


print(fatality_summary)

```

## 4.1 Now analyzing the total fatalities by the road type...

```{r, include=T, results='markup',message=TRUE}

# Sort the summary data by Total Fatalities for better visual clarity
fatality_summary <- fatality_summary %>%
  arrange(desc(Total_Fatalities))

# Enhanced bar plot with additional features
fatality_total_bar_plot <- ggplot(fatality_summary, aes(x = reorder(func_sysname, -Total_Fatalities), y = Total_Fatalities, fill = Total_Fatalities)) +
  geom_bar(stat = "identity", color = "black", width = 0.7) +
  geom_text(aes(label = Total_Fatalities), vjust = -0.5, color = "black", size = 3.5) +  
  
  labs(title = "Total Fatalities by Road Type",
       x = "Road Type",
       y = "Total Number of Fatalities") +
  theme_minimal() +
  scale_fill_gradient(low = "lightblue", high = "darkred") + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

fatality_total_bar_plotly <- ggplotly(fatality_total_bar_plot, tooltip = c("x", "y", "Total_Fatalities")) %>%
  layout(
    xaxis = list(title = "Road Type"),
    yaxis = list(title = "Total Number of Fatalities"),
    width = 900,  
    height = 500  
  )

fatality_total_bar_plotly
```


### 4.2 Statistical Testing

**Since we're now dealing with total counts rather than averages, a chi-square test can be more suitable than an ANOVA to determine if there are statistically significant differences in the distribution of fatalities across road types.**


```{r, include=T, results='markup',message=TRUE}

fatality_table <- table(road_fatalities$func_sysname, road_fatalities$fatals > 0)

chi_square_result <- chisq.test(fatality_table)
print(chi_square_result)

```

## Understanding the results...

#### A large chi-square value, like 36993, indicates a large deviation from this "no-difference" scenario, suggesting that some road types are clearly associated with more fatalities than others.

#### p-value < 2e-16 - A p-value less than 0.05 is typically considered statistically significant, meaning the observed differences are unlikely to be due to random chance. Here, the p-value is much smaller (effectively zero), meaning the differences we're seeing are almost certainly real and not due to random chance.

## What are some of the practical implications?

#### Practical Implications

#### **High-Risk Road Types:**

#### Since fatalities are not randomly distributed, certain road types are inherently riskier. This could mean that these roads need extra safety measures.For example, if highways and major arterials show higher fatalities, these road types might benefit from stricter speed controls, increased patrolling, and improved road signage.

#### **Data-Driven Intervention:**

#### The statistical significance (p-value < 2e-16) gives us confidence that investing in safety improvements on high-fatality road types is likely to make a measurable impact on reducing fatalities. This helps decision-makers focus resources where they are most needed rather than spreading resources equally across all road types.

#### **Targeted Policies:**

#### With evidence that fatalities cluster on certain road types, policies could be customized. For example, highways might benefit from median barriers and crash cushions, while local roads might benefit from better lighting and pedestrian crossings.


### 4.3 **Now, let's identify the top fatal routes by routename and functional system name.**

```{r, include=T, results='markup',message=TRUE}

route_function_summary <- dataset %>%
  group_by(routename, func_sysname) %>%
  summarise(fatals = n()) %>%
  arrange(desc(fatals)) %>%
  ungroup()

# Filter the top 10 high-frequency routes and functional systems
top_route_function_summary <- route_function_summary %>%
  slice_head(n = 10)

```

```{r, include=T, results='markup',message=TRUE}
crash_freq_plot <- ggplot(top_route_function_summary, aes(x = reorder(routename, fatals), y = fatals, fill = func_sysname)) +
  geom_bar(stat = "identity", color = "black", width = 0.7) +
  labs(title = "Top Routes and Func Systems by Crash Freq",
       x = "Route Name",
       y = "Fatals") +
  theme_minimal() +
  scale_fill_brewer(palette = "Paired") + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.title = element_text(size = 10),
    legend.position = "right",
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  coord_flip()  


crash_freq_plotly <- ggplotly(crash_freq_plot, tooltip = c("x", "y", "fill"))

crash_freq_plotly
```

```{r, include=T, results='markup',message=TRUE}
crash_freq_heatmap <- ggplot(top_route_function_summary, aes(x = func_sysname, y = reorder(routename, fatals), fill = fatals)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkred", name = "Crash Count") +
  labs(title = "Crash Frequency Heatmap by Route and Functional System",
       x = "Functional System",
       y = "Route Name") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10)
  )

crash_freq_heatmap_plotly <- ggplotly(crash_freq_heatmap, tooltip = c("x", "y", "fill"))

crash_freq_heatmap_plotly
```

## **Insights**

#### **High Traffic Volume Routes are High-Risk**: Routes like State Highway, US Highway, and Interstate show high crash frequencies, likely due to high traffic volume and higher speeds. This suggests that major highways and state routes may need targeted safety interventions to manage these risks effectively.

#### **Diverse Functional Systems on Certain Routes**: The presence of multiple functional systems (e.g., Principal Arterial and Minor Arterial) on State Highways and Local Streets indicates that these roads experience varied traffic conditions, which could contribute to higher crash risks.

## **Potential for Safety Interventions**:

#### State and US Highways could benefit from interventions such as speed control measures, improved signage, and roadway design enhancements. Local streets and county roads, while having fewer crashes overall, may still need safety measures, especially in urban areas with pedestrian traffic.


## 4.4 **Do fatalities and frequency differ significantly between urban and rural functional systems?**

```{r, include=T, results='markup',message=TRUE}

filtered_dataset <- dataset %>%
  filter(!func_sysname %in% c("Unknown", "Not Reported", "Trafficway Not in State Inventory"),
         !rur_urbname %in% c("Unknown", "Not Reported", "Trafficway Not in State Inventory"))

# Summarize crash severity and frequency by functional system and urban/rural classification
summary_data <- filtered_dataset %>%
  group_by(rur_urbname, func_sysname) %>%
  summarise(
    Avg_Severity = mean(fatals, na.rm = TRUE),
    Crash_Frequency = n()
  ) %>%
  ungroup()

for (func_sys in unique(summary_data$func_sysname)) {
  
  subset <- summary_data %>%
    filter(func_sysname == func_sys)
  
  # Plot for Average Crash Severity with Urban and Rural together
  severity_plot <- ggplot(subset, aes(x = rur_urbname, y = Avg_Severity, fill = rur_urbname)) +
    geom_bar(stat = "identity", position = "dodge", color = "black") +
    labs(title = paste("Average Crash Severity by Area Type -", func_sys),
         x = "Area Type (Urban/Rural)",
         y = "Avg. Severity") +
    scale_fill_manual(values = c("Urban" = "skyblue", "Rural" = "salmon")) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      legend.position = "none"
    )
  
  print(severity_plot)
  
  # Plot for Crash Frequency with Urban and Rural together
  frequency_plot <- ggplot(subset, aes(x = rur_urbname, y = Crash_Frequency, fill = rur_urbname)) +
    geom_bar(stat = "identity", position = "dodge", color = "black") +
    labs(title = paste("Crash Frequency by Area Type -", func_sys),
         x = "Area Type (Urban/Rural)",
         y = "Frequency") +
    scale_fill_manual(values = c("Urban" = "lightgreen", "Rural" = "darkorange")) +
    theme_minimal(base_size = 10) +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      legend.position = "none"
    )
  
  print(frequency_plot)
}
```

### **Are there any noticeable trends in fatal accidents over the years?**

## Graphical Representations

### In this section, we present visualizations to better understand the relationship between EMS arrival times and the number of fatalities. 


### We include a boxplot illustrating the distribution of fatalities by month. By converting the `monthname` into a factor with a defined order, we ensure that the boxplot accurately reflects the seasonal variations in fatalities. This visualization highlights any monthly patterns or trends in fatal accidents, allowing for a clearer understanding of when fatalities are most likely to occur throughout the year.

```{r graphical-representations}
# Create total_fatalities based on the fatals column
dataset$total_fatalities <- ifelse(dataset$fatals > 0, 1, 0)

# Boxplot of Fatalities by Month
ggplot(dataset, aes(x=factor(monthname, levels=month.name), y=fatals)) +  # Convert monthname to a factor
    geom_boxplot(fill="lightgreen") +
    labs(title="Boxplot of Fatalities by Month", x="Month", y="Total Fatalities") +
    theme_minimal()
```

### It shows that there are significant fluctuations in fatality counts throughout the year, with certain months like Jan, March, July indicating higher median fatalities compared to others. 

### Normality Test

### In this section, we evaluate the distribution of the total fatalities to determine if it follows a normal distribution. First, we summarize the `total_fatalities` variable, which aggregates the fatalities from the dataset. We then perform the Shapiro-Wilk normality test to assess whether the data is normally distributed. A Q-Q plot is also generated to visually inspect the normality of the data distribution, helping us understand the underlying characteristics of the fatalities.

```{r normality-tests}
str(dataset)

dataset$total_fatalities <- dataset$fatals 

non_missing_count <- sum(!is.na(dataset$total_fatalities))
cat("Number of non-missing values in total_fatalities:", non_missing_count, "\\n")

unique_values <- unique(dataset$total_fatalities)
summary(dataset$total_fatalities)
cat("Unique values in total_fatalities:", unique_values, "\\n")

if (non_missing_count >= 3 && non_missing_count <= 5000) {
    shapiro_test <- shapiro.test(dataset$total_fatalities)
    cat("Shapiro-Wilk Normality Test:\\n")
    print(shapiro_test)
} else {
    cat("Insufficient sample size for Shapiro-Wilk test.\\n")
}

qqnorm(dataset$total_fatalities, main="Q-Q Plot for Total Fatalities")
qqline(dataset$total_fatalities, col="red")
```
### The Q-Q plot displays the quantiles of the sample data (total fatalities) against the quantiles of a theoretical normal distribution. If the data points closely follow the red line, it indicates that the data is normally distributed.

## Statistical Test

### Chi-Square Test
```{r chi-square-test}
table_states <- table(dataset$statename)

chi_square_test <- chisq.test(table_states)

cat("Chi-Square Test Results:\n")
print(chi_square_test)
```

### The Chi-Square test results indicate a significant association between the categorical variable and the observed fatalities, with a Chi-squared statistic of 52686 and a p-value of less than 2e-16. This strong evidence against the null hypothesis suggests that fatalities are not uniformly distributed across the categories analyzed, which would require further analysis.

## Monthly trends for 2022

# Analyzing fatalities by month to identify seasonal patterns
```{r}
monthly_summary <- dataset %>% 
  group_by(monthname) %>% 
  summarize(total_fatalities = sum(fatals, na.rm = TRUE), .groups = 'drop') 

monthly_summary$monthname <- factor(monthly_summary$monthname, levels = month.name)

ggplot(monthly_summary, aes(x=monthname, y=total_fatalities)) +
  geom_line(group=1, color="blue", size=1.2) +
  geom_point(color="red", size=3) +
  labs(title="Trends in Fatal Accidents by Month (2022)",
       x="Month",
       y="Total Fatalities") +
  theme_minimal()
```

### The line graph illustrates the monthly trends in fatal accidents for the year 2022. The data reveals fluctuations in the total number of fatalities, with the highest peak in October, suggesting that this month experienced a higher incidence of fatal accidents compared to others. Conversely, February shows the lowest total fatalities, indicating a potential seasonal variation in accident frequency. The overall upward trend in the latter half of the year indicates that fatalities may increase during certain months, which could be influenced by various factors such as weather conditions, holiday traffic, or enforcement of traffic laws.


### Regional Analysis for the year 2022

```{r, fig.width=14, fig.height=10}
# Summarizing total fatalities by state and month
state_monthly_summary <- dataset %>% 
  group_by(statename, monthname) %>%  # Ensure correct column names
  summarize(total_fatalities = sum(fatals, na.rm = TRUE), .groups = 'drop')

# Display the head of the summary
print(head(state_monthly_summary))

# Create a factor for monthname to ensure it is ordered correctly
state_monthly_summary$monthname <- factor(state_monthly_summary$monthname, levels = month.name)

# Plotting state-wise trends in fatalities by month
ggplot(state_monthly_summary, aes(x=monthname, y=total_fatalities, color=statename, group=statename)) +
  geom_line(size=1, alpha=0.7) +  
  geom_point(size=2, alpha=0.8) +  
  scale_y_continuous(breaks = seq(0, max(state_monthly_summary$total_fatalities, na.rm = TRUE), by = 10)) + 
  labs(title="State-wise Trends in Fatal Accidents by Month",
       x="Month",
       y="Total Fatalities",
       color="State") +
  theme_minimal() +
  theme(legend.position = "bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1), 
        panel.grid.minor = element_blank()) 

# Save the plot as a PNG file
ggsave("enlarged_state_trends.png", width = 18, height = 12, dpi = 300)
```

### The line graph illustrates the monthly trends in fatal accidents across various states in 2022. Each line represents a different state, allowing for a comparative analysis of how fatalities fluctuate throughout the year.

### **Trends and Variations**: The graph shows significant variability among states in the number of fatal accidents. For instance, states like Texas and California may show higher fatalities during summer months, while others may experience peaks during different times of the year.

### **Seasonal Patterns**: Some states exhibit clear seasonal patterns, with increases in fatalities during certain months, possibly due to factors like weather conditions, travel patterns, or seasonal activities that affect road usage.

### **Comparative Insights**: This visualization allows stakeholders to identify which states have persistently high fatalities and to examine the effectiveness of traffic safety measures. States with rising trends may need targeted interventions to reduce accidents.



### Identify the region with the highest fatalities in each month

```{r}
regional_monthly_summary <- dataset %>% 
  group_by(statename, monthname) %>%  
  summarize(total_fatalities = sum(fatals, na.rm = TRUE), .groups = 'drop')

high_risk_states <- regional_monthly_summary %>% 
  group_by(monthname) %>% 
  slice(which.max(total_fatalities)) %>% 
  select(monthname, statename, total_fatalities)

cat("States with the highest number of fatalities for each month:\n")
print(high_risk_states)
```
### **Highest Fatalities**: December shows the highest total fatalities in Texas (414), suggesting that winter holidays may contribute to increased road usage and accidents. Similarly, California has notably high fatalities in October (406) and August (404), indicating potential summer travel spikes and events that may influence accident rates.

### **Comparative Trends**: California consistently reports high fatalities across several months (April, February, March), highlighting a potential trend of elevated risk for road accidents in this state. Conversely, Texas shows a fluctuating pattern, with high fatalities in both winter (January, December) and summer (July, May).

### This analysis underscores the need for targeted traffic safety interventions, particularly during months identified with high fatality counts.

# Checking if the no.of fatalities have been decreasing or increasing over the months in different regions


```{r}
regional_trends <- dataset %>% 
  group_by(statename, monthname) %>%
  summarize(total_fatalities = sum(fatals, na.rm = TRUE), .groups = 'drop')

regional_trends <- regional_trends %>%
  mutate(month_numeric = match(monthname, month.name)) 

trend_direction <- regional_trends %>%
  group_by(statename) %>%
  filter(n() > 1) %>% 
  summarize(trend = cor(month_numeric, total_fatalities, use = "complete.obs")) %>%
  mutate(direction = ifelse(trend > 0, "Increasing", "Decreasing"))

cat("Trend Analysis Summary for Fatalities in 2022:\n")
print(trend_direction)
```


### **General Increase**: A majority of states (e.g., Alabama, Alaska, Arizona) show a positive growth rate in fatalities, suggesting a concerning upward trend in traffic-related deaths.

### **States with Decreasing Trends**: Notably, a few states, such as the District of Columbia, Florida, Hawaii, and Nebraska, exhibit a decrease in fatalities. Florida and Hawaii decreases by 0.2377 and 0.3483, respectively.

### **High Variability in Change Rates**: Some states, like Missouri (0.6856) and Pennsylvania (0.6936), have particularly high increasing rates. Conversely, there are states with a slight increase, such as Georgia (0.1040).


## How does the EMS arrival time influence the severity of fatal outcomes?

### We will explore how EMS response times correlate with the severity of fatal accidents by categorizing accidents into different severity levels based on the number of fatalities.

### EMS Response Time Calculation: Calculating the total EMS arrival time in minutes using `ARR_HOUR` and `ARR_MIN`. 
### **Conversion of Fatality Count**: Ensuring the `FATALS` column is numeric for analysis. 
### **Categorize Fatality Severity**: Creating a new column for severity levels based on the number of fatalities.

```{r data-preparation}
# Calculate EMS arrival time in minutes using only the minute column
dataset$ems_arrival_time <- as.numeric(dataset$arr_min)

# Check if FATALS column exists and convert it to numeric
if ("fatals" %in% colnames(dataset)) {
    dataset$fatals <- as.numeric(dataset$fatals)
} else {
    stop("FATALS column is missing in the dataset.")
}

# Categorize fatal accidents into severity levels
dataset$severity_level <- cut(
  dataset$fatals,
  breaks = c(0, 1, 3, 5, Inf),
  labels = c("Low (1 Fatality)", "Medium (2-3 Fatalities)", "High (4-5 Fatalities)", "Very High (6+ Fatalities)")
)

# Overview of the EMS-related data after preparation
glimpse(dataset[, c("ems_arrival_time", "fatals", "severity_level")])
summary(dataset[, c("ems_arrival_time", "fatals", "severity_level")])

```
### **EMS Arrival Time**: The ems_arrival_time ranges from 0 to 99 minutes, with a median of 30 minutes and a mean of approximately 29.1 minutes. This indicates that most EMS responses occur relatively quickly, although the presence of a few instances with 0 minutes suggests there may be some data entry issues or unusual cases where arrival times were not recorded properly.

### **Fatalities**: The fatals column ranges from 1 to 9, with a minimum of 1 fatality in nearly all cases, leading to a mean of 1.08 fatalities per incident. This reflects a predominance of single-fatality incidents, indicating that most accidents involve one death, while multi-fatality events are rare.

### **Severity Levels**: The categorization of severity levels shows that 93.1% of incidents fall under the "Low (1 Fatality)" category, with Medium (2-3 Fatalities) representing 6.5%. The High (4-5 Fatalities) and Very High (6+ Fatalities) categories are very infrequent, suggesting that while serious accidents do occur, they are outliers in this dataset.

### Descriptive Statistics

### We will examine basic statistics for EMS arrival times and fatalities, including measures of central tendency and variability.

```{r descriptive-statistics}
ems_stats <- dataset %>%
  summarize(
    mean_arrival_time = mean(ems_arrival_time, na.rm = TRUE),
    median_arrival_time = median(ems_arrival_time, na.rm = TRUE),
    sd_arrival_time = sd(ems_arrival_time, na.rm = TRUE),
    min_arrival_time = min(ems_arrival_time, na.rm = TRUE),
    max_arrival_time = max(ems_arrival_time, na.rm = TRUE),
    mean_fatalities = mean(fatals, na.rm = TRUE),
    max_fatalities = max(fatals, na.rm = TRUE)
  )
print(ems_stats)
```
### **Mean Arrival Time**: The mean EMS arrival time is 29.1 minutes, indicating that, on average, it takes just under half an hour for EMS to respond to incidents. This statistic gives an overall sense of response efficiency across the dataset.

### **Median Arrival Time**: The median arrival time of 30 minutes suggests that half of the responses occur within this timeframe, confirming that a significant portion of incidents sees relatively prompt EMS response.

### **Standard Deviation**: With a standard deviation of 18.1 minutes, there is notable variability in response times. This implies that while many responses are timely, there are instances where responses are significantly delayed, warranting further investigation into factors causing such variability.

### **Minimum Arrival Time**: The minimum arrival time recorded is 0 minutes, which may indicate instances where the arrival time was not properly documented or possibly reflects cases where EMS was already present at the scene. This could signal data entry issues that need addressing.



### EMS Arrival Time by Severity Level

### We will compare EMS response times for different severity levels to see if delays correlate with more severe outcomes.

```{r severity-analysis}
ggplot(dataset, aes(x=severity_level, y=ems_arrival_time, fill=severity_level)) +
  geom_boxplot(alpha=0.6) +
  labs(title="EMS Arrival Time by Fatality Severity Level",
       x="Severity Level",
       y="EMS Arrival Time (minutes)",
       fill="Severity Level") +
  theme_minimal()
```

### This boxplot analysis highlights important trends in EMS arrival times related to the severity of fatalities. It underscores the need for continued efforts to improve response times, particularly in high-severity cases, to ensure better emergency care outcomes.

###Correlation Analysis of Severity

### We will explore the correlation between EMS arrival times and the severity of accidents.

```{r severity-correlation}
correlation_severity <- cor.test(dataset$ems_arrival_time, dataset$fatals, use="complete.obs")
cat("Correlation between EMS Arrival Time and Severity (Number of Fatalities):\n")
print(correlation_severity)
```
### **Correlation Coefficient**: The Pearson correlation coefficient is -0.00754, indicating a very weak negative correlation between EMS arrival time and the number of fatalities.

### **Statistical Significance**: The p-value is 0.1, which is greater than the conventional alpha level of 0.05. This indicates that the correlation is not statistically significant, meaning we do not have enough evidence to reject the null hypothesis that states there is no correlation between the two variables.

### **Confidence Interval**: The 95% confidence interval for the correlation coefficient ranges from -0.01761 to 0.00254. 


### Linear Regression Analysis

### Perform a linear regression to determine if EMS arrival time is a predictor of the number of fatalities.

```{r linear-regression}
linear_model <- lm(fatals ~ ems_arrival_time, data = dataset)
summary(linear_model)

ggplot(dataset, aes(x = ems_arrival_time, y = fatals)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Linear Regression: EMS Arrival Time vs Number of Fatalities",
       x = "EMS Arrival Time (minutes)",
       y = "Number of Fatalities") +
  theme_minimal()
```


## Advanced Statistical Analysis

### ANOVA for Severity Levels
We will use an ANOVA test to see if there's a statistically significant difference in EMS arrival times across different severity levels.

```{r anova-analysis}
anova_result <- aov(ems_arrival_time ~ severity_level, data = dataset)
summary(anova_result)

TukeyHSD(anova_result)
```
### **ANOVA Results**: The ANOVA summary indicates that the F-value is 1.01 with a corresponding p-value of 0.39. Since the p-value exceeds the common alpha level of 0.05, we fail to reject the null hypothesis, concluding that there is no statistically significant difference in EMS arrival times across the different severity levels.

### The mean arrival time difference between "Medium (2-3 Fatalities)" and "Low (1 Fatality)" is -0.144, with a confidence interval ranging from -1.11 to 0.821, indicating that the differences are not significant.

### Larger differences are observed when comparing "Very High (6+ Fatalities)" with "Low (1 Fatality)" at -7.659, with a confidence interval from -24.06 to 8.745, again demonstrating no significant difference given the p-value of 0.627.

### ANOVA analysis and subsequent Tukey post-hoc tests reveal that there are no statistically significant differences in EMS arrival times across the various severity levels of fatalities.

### Logistic Regression Analysis

### We will perform logistic regression to see if EMS arrival time influences the probability of a high-severity outcome.

```{r logistic-regression}
dataset$high_severity <- ifelse(dataset$fatals > 3, 1, 0)

logistic_model <- glm(high_severity ~ ems_arrival_time, data = dataset, family = binomial)
summary(logistic_model)

exp(coef(logistic_model))
```
### The null deviance is 1643.5, and the residual deviance is 1641.3, suggesting that the model fits the data reasonably well.
### **Intercept**: The estimate of -5.52464 indicates the log-odds of a high-severity incident when EMS arrival time is zero. 
### **EMS Arrival Time**: The coefficient for ems_arrival_time is -0.00750, suggesting that for each additional minute in EMS arrival time, the log-odds of experiencing a high-severity incident decrease slightly. 
### **Model Fit Statistics**: The AIC (Akaike Information Criterion) of 1645 provides a metric for comparing the relative quality of this model with others; lower values typically indicate a better-fitting model.


### Summary and Insights

### Based on the analysis, summarize the key observations and their implications for EMS response effectiveness.

```{r summary-findings}
cat("Summary of Findings:\n")
if (correlation_severity$p.value < 0.05) {
  cat("- The analysis indicates a significant relationship between EMS arrival time and the severity of accidents.\n")
  if (correlation_severity$estimate > 0) {
    cat("- Longer EMS response times are associated with more severe fatal outcomes.\n")
  } else {
    cat("- Shorter EMS response times are associated with less severe fatal outcomes.\n")
  }
} else {
  cat("- There is no significant relationship between EMS arrival time and the severity of accidents.\n")
}
```

## Key Findings:
#### Fatal accidents are more frequent between 6 pm and 10 pm and on weekends, with late-night hours linked to increased severity
#### Clear weather conditions see the highest fatalities, while poorly lit roads (Dark - Not Lighted) have elevated risks.
#### Highways and major arterials report the most fatalities, likely influenced by higher speeds and traffic volume.
#### EMS response time did not significantly affect fatality severity, suggesting a greater need for preventive measures over emergency response.

## Limitation
#### Solely focus on accident file, excluding critical behavioral aspects like impaired driving or motorist fatigue

## Further considerations: Cross Factor Analysis
#### To gain more comprehensive understanding, further analysis should integrate these factors, examining the interactive effect on fatal accidents
#### investigate the underlying explanation for our analysis result
